{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "input_file = '../../sources/merged_gutenberg_source_files.txt'\n",
    "output_file = 'output/merged_gutenberg_tokenized.txt'\n",
    "\n",
    "with open(input_file, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Set the mode ('complete' or 'novels')\n",
    "mode = 'novels'  # Change this to 'novels' if needed\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Define your regular expression for the title markers\n",
    "title_marker = re.compile(r'\\[[A-Z\\']+\\]')\n",
    "\n",
    "# If analyzing the entire corpus, you can remove the title markers\n",
    "if mode == 'complete':\n",
    "    tokens = [token for token in tokens if not title_marker.match(token)]\n",
    "    # Save tokenized text to 'tokenized_complete.txt'\n",
    "    with open(output_file, 'w'):\n",
    "        output_file.write(' '.join(tokens))\n",
    "\n",
    "# If analyzing individual novels, you can split the tokens list into sublists\n",
    "elif mode == 'novels':\n",
    "    novels = []\n",
    "    novel = []\n",
    "    for token in tokens:\n",
    "        if title_marker.match(token):\n",
    "            if novel:  # if the novel list is not empty, add it to novels\n",
    "                novels.append(novel)\n",
    "                novel = []  # start a new novel\n",
    "        else:\n",
    "            novel.append(token)\n",
    "    \n",
    "    if novel:  # add the last novel\n",
    "        novels.append(novel)\n",
    "    \n",
    "    # Save tokenized novels to 'tokenized_novels.txt'\n",
    "    with open(output_file, 'w') as f:\n",
    "        for novel_tokens in novels:\n",
    "            f.write(' '.join(novel_tokens) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check that the file has been tokenized. \n",
    "\n",
    "def is_tokenized(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        first_line = f.readline()\n",
    "        # If the first line contains spaces, it's likely the file has been tokenized\n",
    "        return ' ' in first_line\n",
    "\n",
    "print(is_tokenized(output_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
